{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-08-10 21:43:21,928] Making new env: CartPole-v0\n",
      "[2016-08-10 21:43:21,957] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-08-12 17:50:33,663] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 16 timesteps\n",
      "Episode finished after 68 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 18 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        time.sleep(.05)\n",
    "        env.render()\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--seed SEED] [--iters ITERS] [--samples SAMPLES]\n",
      "                   [--num_steps NUM_STEPS] [--top_frac TOP_FRAC]\n",
      "                   [--algorithm {pcem,cem}] [--outdir OUTDIR] [--render]\n",
      "                   [--upload]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-be7ef673-ddbc-4d31-9131-9a4aea4bf0cb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'level' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/michael/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2875\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2877\u001b[0;31m             \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To exit: use 'exit', 'quit', or Ctrl-D.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2878\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m             \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'level' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "\"\"\"The main idea of CE (Cross Entropy) is to maintain a distribution\n",
    "of possible solution, and update this distribution accordingly.\n",
    "Preliminary investigation showed that applicability of CE to RL problems\n",
    "is restricted severly by the phenomenon that the distribution concentrates to\n",
    "a single point too fast.\n",
    "To prevent this issue, noise is added to the previous stddev/variance update\n",
    "calculation.\n",
    "We implement two algorithms cem, the Cross-Entropy Method (CEM) with noise [1] and\n",
    "Proportional Cross-Entropy (PCEM) [2].\n",
    "CEM is implemented with decreasing variance noise\n",
    "    variance + max(5 - t / 10, 0), where t is the iteration step\n",
    "PCEM is implemented the same as CEM except we adjust the weights, evaluations of f\n",
    "as follows:\n",
    "    M = max(weights)\n",
    "    m = min(weights)\n",
    "    weights = (weight - m) / (M - m + eps)\n",
    "    where eps is a very small value to avoid division by 0\n",
    "An issue with CEM is it might not optimize the actual objective. PCEM helps\n",
    "with this.\n",
    "References:\n",
    "    [1] Learning Tetris with the Noisy Cross-Entropy Method (Szita, Lorincz 2006)\n",
    "    [2] The Cross-Entropy Method Optimizes for Quantiles (Goschin, Weinstein, Littman 2013)\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# only two possible actions 0 or 1\n",
    "class BinaryActionLinearPolicy(object):\n",
    "    def __init__(self, theta):\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)\n",
    "        return a\n",
    "\n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    \"\"\"\n",
    "    Performs actions for num_steps on the environment\n",
    "    based on the agents current params\n",
    "    \"\"\"\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_rew, t+1\n",
    "\n",
    "# mean and std are 1D array of size d\n",
    "def cem(f, mean, var, n_iters, n_samples, top_frac):\n",
    "    top_n = int(np.round(top_frac * n_samples))\n",
    "    for i in range(n_iters):\n",
    "        # generate n_samples each iteration with new mean and stddev\n",
    "        samples = np.transpose(np.array([np.random.normal(u, np.sqrt(o), n_samples) for u, o in zip(mean, var)]))\n",
    "        ys = np.array([f(s) for s in samples])\n",
    "        # the top samples are the ones which give the lowest f evaluation results\n",
    "        top_idxs = ys.argsort()[::-1][:top_n]\n",
    "        top_samples = samples[top_idxs]\n",
    "        # this is taken straight from [1], constant noise param\n",
    "        # dependent on the iteration step.\n",
    "        v = max(5 - i / 10, 0)\n",
    "        mean = top_samples.mean(axis=0)\n",
    "        var = top_samples.var(axis=0) + v\n",
    "        yield {'ys': ys, 'theta_mean': mean, 'y_mean': ys.mean()}\n",
    "\n",
    "def pcem(f, mean, var, n_iters, n_samples, top_frac):\n",
    "    eps = 1e-10 # avoid dividing by 0\n",
    "    top_n = int(np.round(top_frac * n_samples))\n",
    "    for i in range(n_iters):\n",
    "        # generate n_samples each iteration with new mean and stddev\n",
    "        samples = np.transpose(np.array([np.random.normal(u, np.sqrt(o), n_samples) for u, o in zip(mean, var)]))\n",
    "        ys = np.array([f(s) for s in samples])\n",
    "        max_y = np.max(ys)\n",
    "        min_y = np.min(ys)\n",
    "        ys = (ys - min_y) / (max_y - min_y + eps)\n",
    "        # the top samples are the ones which give the lowest f evaluation results\n",
    "        top_idxs = ys.argsort()[::-1][:top_n]\n",
    "        top_samples = samples[top_idxs]\n",
    "        # this is taken straight from [1], constant noise param\n",
    "        # dependent on the iteration step.\n",
    "        v = max(5 - i / 10, 0)\n",
    "        mean = top_samples.mean(axis=0)\n",
    "        var = top_samples.var(axis=0) + v\n",
    "        yield {'ys': ys, 'theta_mean': mean, 'y_mean': ys.mean()}\n",
    "\n",
    "\n",
    "def evaluation_func(policy, env, num_steps):\n",
    "    def f(theta):\n",
    "        agent = policy(theta)\n",
    "        rew, t = do_rollout(agent, env, num_steps, render=False)\n",
    "        return rew\n",
    "    return f\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "    parser.add_argument('--iters', default=50, type=int, help='number of iterations')\n",
    "    parser.add_argument('--samples', default=30, type=int, help='number of samples CEM algorithm chooses from on each iter')\n",
    "    parser.add_argument('--num_steps', default=200, type=int, help='number of steps/actions in the rollout')\n",
    "    parser.add_argument('--top_frac', default=0.2, type=float, help='percentage of top samples used to calculate mean and variance of next iteration')\n",
    "    parser.add_argument('--algorithm', default='cem', type=str, choices=['pcem', 'cem'])\n",
    "    parser.add_argument('--outdir', default='CartPole-v0-cem', type=str, help='output directory where results are saved (/tmp/ prefixed)')\n",
    "    parser.add_argument('--render', action='store_true', help='show rendered results during training')\n",
    "    parser.add_argument('--upload', action='store_true', help='upload results via OpenAI API')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    env = gym.make('CartPole-v0')\n",
    "    num_steps = args.num_steps\n",
    "\n",
    "    ef = None\n",
    "    if args.algorithm == 'cem':\n",
    "        ef = cem\n",
    "    else:\n",
    "        ef = pcem\n",
    "\n",
    "    outdir = '/tmp/' + args.outdir\n",
    "    env.monitor.start(outdir, force=True)\n",
    "\n",
    "    f = evaluation_func(BinaryActionLinearPolicy, env, num_steps)\n",
    "\n",
    "    # params for cem\n",
    "    params = dict(n_iters=args.iters, n_samples=args.samples, top_frac=args.top_frac)\n",
    "    u = np.random.randn(env.observation_space.shape[0]+1)\n",
    "    var = np.square(np.ones_like(u) * 0.1)\n",
    "    for (i, data) in enumerate(ef(f, u, var, **params)):\n",
    "        print(\"Iteration {}. Episode mean reward: {}\".format(i, data['y_mean']))\n",
    "        agent = BinaryActionLinearPolicy(data['theta_mean'])\n",
    "        if args.render:\n",
    "            do_rollout(agent, env, num_steps, render=True)\n",
    "\n",
    "    env.monitor.close()\n",
    "    # make sure to setup your OPENAI_GYM_API_KEY environment variable\n",
    "    #if args.upload:\n",
    "    #    gym.upload(outdir, algorithm_id=args.algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
